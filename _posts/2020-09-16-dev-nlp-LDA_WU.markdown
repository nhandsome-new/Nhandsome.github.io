---
layout: post
title:  "[LDA] LDAを理解するためのウォーミングアップ"
subtitle:   "Subtitle - Warming up for understanding LDA"
categories: dev
tags: nlp
comments: true
---
## Describe
> `LDA`を勉強しようとすると、<u>ベイズ推定・Dirichlet分布など</u>様々な概念が出てきたり複雑な数式が出てきたりし、難しく感じられる。
このポスティングでは、`数学的な話を除いたLDAへの流れ`を説明します。

## 目次
- [入る前に](#jump1)
- [何を勉強するの？](#jump2)
  - [サンプリングの理解](#jump3)
  - [概念整理：ベイズ推定](#jump4)
  - [概念整理：Dirichlet分布](#jump5)
  - [概念整理：LDA](#jump6)
  - [Gibbs Sampling](#jump7)
  - [LDAへの適用](#jump8)
- [参考資料](#jump9)

<br><br><br>


## <a name="jump1">入る前に</a>
---
> 　この資料は`ベイズ推定・Dirichlet分布・MCMCパラダイムの説明`を聞いてから自分が作った資料です。申し上げた概念はググってみると沢山の情報があり、個別の概念を理解するのは難しくはないと思います。（僕には難しいでした）<br>
　ここでは詳しい概念の説明や数学的な話は出来るだけ話さず、`なぜLDAを勉強するのにあいう概念を勉強しなければならないの`という観点で話を進めたい思います。　

<br><br><br>


## <a name="jump2">何を勉強するの？</a>
---
![目次](/assets/img/dev/gibbs_1.jpg)
そもそも考えたテーマは`Gibbs Sampling`を理解するっということでしたが、最終的には
* **サンプルリングって何だろう？　なぜ使うの？**
* **ベイズ推定・Dirichlet分布ってなぜ勉強したの？**
* **Gibbs Samplingは何だろう？**
* **あいうこととLDAはどんな関係なの？**
を考えてみることになりそう…です。

> LDAってこういう流れやな…ということを理解するために作ったものですが、<u>厳しく見ると間違った説明もあるかもしれません。是非コメント残してください。</u>

<br><br><br>


## <a name="jump3">サンプリングの理解</a>
---
![サンプリング１](/assets/img/dev/gibbs_2.jpg)
`母集団の情報を得るため,個別な観測を行う統計的なプロセス`とWikipediaに書かれています。なんかピンと来なくて,
とりあえず`平均μのNormal分布に従うxを上手く抽出してみよう`と思い、上記のプロセス①〜③を考えてみました。気づいたことは次のとおりです。
* **よく知られているNormal分布なのにこんな複雑な問題になるって…**
* **手書きで解くのはなかなか難しいかも…**
* **積分出来ない関数であれば、サンプリングどうするの？**

> <u>いきなりp(x)の情報をもらったとしても、それに従うランダム数を抽出するのは難しいんだ？</u><br>
`p(x)が知られているのが統計的な問題が全部簡単に解けるという意味ではないんだ`
<br>

![サンプリング２](/assets/img/dev/gibbs_3.jpg)
`確率分布p(x)が知られているが、その式が複雑（高次元・多変数）`であれば、積分して逆関数を求めて、サンプリングするのは不可能であります。<br>
その解決方法として`Markov chain Monte Carlo(MCMC)`が上げられてます。詳しい説明はジャンプして、`山登りの例`でMCMCをザックリ理解してみましょう。(上記①〜④のプロセス)
また次のことを考えてみましょう。
* **現在位置・周りのある位置の高さが比較できる　＝　p(x)とp(x')の比率計算ができる**
* **移動して1分間過ごす　＝　xをサンプリングする**
* **各位置で過ごした時間を合算する　＝　xをサンプリングした総回数を計算する**

> 確かに、`現在位置・周りのある位置の高さを比較するだけ`で全ての山をぶらぶらしたり、頂上を位置を探したりはしてないな…<u>数学として積分のような複雑な探査はしてない気がする。</u>

<br>

![サンプリング３](/assets/img/dev/gibbs_4.jpg)
　上の図を見てみると`山登りプロセスを繰り返せば結果が美しくなる`ことが分かります。金色の山の形は知らなかったはずですが、<u>過ごした時間の総合は山の形</u>を表していますね。
ちょうど前のページで述べた話を貸すと、`p(x)の積分が出来なくても式さえ分かれば、その確率分布に従うランダム抽出ができる`ということになりそうです。

> 実際には色んな数式や証明が含まれている部分ですが、山登りの例を通じ`p(x)書いているのになんでえらそうにサンプリングということ使うの`という疑問を弱くさせることに注目しました。
僕もまだ詳しい数式・証明過程は理解していませんが、なんかこの考えにたどりついてからは<u>数式がちょっと優しく見えるようになりました。</u>（本当にちょっと）

* **そもそも`正規化（積分が必要）させ、逆関数を使う`とある確率分布に従うデータ抽出ができそうだが、**
* **複雑な確率分布では`正規化が難しくなり、簡単なサンプリング方法が必要`になる。**
* **`MCMCは今の状況(x)よりありそうな状況(x')を探す`サンプリング方法論であり、**
* **`正規化せず`に知っている確率情報p(x)をより簡単に使う。**　ぐらいに理解しておいて、次に進みましょう。

<br><br><br>


## <a name="jump4">概念整理：ベイズ推定</a>
---
![ベイズ１](/assets/img/dev/gibbs_5.jpg)
　LDAを勉強しようと思ったらいつの間にか存在感をだすもの、`ベイズ推定・Dirichlet分布`です。**詳しく勉強してみようと思ったら、難しくて難しくて辞めてしまい…　概念だけさっさとみようとしたら、何を言っているか訳わからず終わってしまう…**謎の存在であると、個人的には思います。<br>
　ここでは`LDAを理解することを目標として、必要な概念・ザックリした感覚だけ`をみていただきたいと思います。実際に専門的で詳しい情報はググってみたらたくさん出てくるし、こういう流れを説明してくれるブログはあんまり無さそうですので…気軽に行ってみましょう。

<br>

![ベイズ２](/assets/img/dev/gibbs_6.jpg)
　ベイズ推定の公式や詳しい説明を扱っている資料は数えられないくらい、たくさんあると思います。また、僕としてはそれを説明するには物足りないので…数式はほっと置いて、`「今の知識 x 新たな観察 → 新たな知識」というベイズ推定の概念だけ`持って前に進みます。`曲がったコインの表が出る確率`を推定してみます。

> **左側の3つの図**
1. 今の知識：<u>どんな形で曲がっているか想像つかないので、</u>
 * 表が出る確率は、「０〜１」のUniform分布であると考える。<br>(0.1であっても0.9であってもおかしくないと思う。)
 * Uniform分布は`Beta(1,1)と同じ形`である。
1. 新たな観測：<u>実際に「３回」投げてみたら、表が「２回」出た。</u>
 * 「表・裏」2つの結果で決まっている実験なので、`Binomialに従う。`
 * Binomial分布は、キレイな公式があり、グラフも描ける。
1. 新たな知識：<u>Beta分布とBinomialの掛け算、めんどくさいけどこれやるのがベイズ推定だから…やってみたら、</u>
 * 何と無くキレイな公式が出ちゃい、`Beta(3,2)に従う。`
 * このコインは表が出る確率が「0.6~0.8」になりそうっと推定できる。

　要するに、`「今の知識 x 新たな観察 → 新たな知識」`というベイズ推定を使おうと思ったら,`「新たな観察(実験)の形がBinomial分布に従うと、その掛け算さえ要らなくなっちゃった。」`というマジックであります。これから私たちがする作業は、`Binomial分布の掛け算ではなく、Beta分布のパラメタの足し算です。`

> **右側の3つの図**
1. 今の知識：<u>前回のベイズ推定の「新たな知識が今の知識」である。</u>
 * `Beta(3,2)`であることを既にわかっている。
1. 新たな観察：<u>「2回」実験してみたら、表が「1回」出た。</u>
 * `Binomial分布`に従う実験であるのをわかっている。
1. 新たな知識：<u>計算しなくても、同然Beta分布になるんでろう。</u>
 * 表1回、裏1回が出たので、`Beta(3+1,2+1) = Beta(4,3)`分布に従う推定ができる。


* **いわゆる`Posterior ∝ Prior x Likelihood`という数式のザックリした概念であり、**
* **`Binomial的なイベントを観察`するなら`Beta分布との組み合わせ`で計算が相当楽になる。**
* **また、Beta分布とBinomial分布の関係を`Conjugate・Conjugation`と言う。**


<br><br><br>

## <a name="jump5">概念整理：Dirichlet分布</a>
---
![Dirichlet](/assets/img/dev/gibbs_7.jpg)
　いよいよDirichletの話になりました。詳細な証明や数学的な話になると、めっちゃ難しい部分になりますが、[前回](#jump4)の`ベイズ推定 ・ Binomial分布 ・ Beta分布`の関係を考えるとすると、この部分は(あくまでザックリした概念)ただ得られるものです。

<br>

|**イベントの数**|**イベントの動き**|**特になる分布**|**例**|
|:---:|:---:|:---:|:---:|
|2|`Binomial`|`Beta`|コイン|
|3以上|`Multinomial`|`Dirichlet`|サイコロ・ジャンケン|

<br>
　コインを投げる実験は、「表か裏」2つのイベントが観察できます。しかし、世の中、2つにはっきり分けられる問題ってあんまり無さそうですね…　「サイコロ」を投げると6つの出来事があり、「ジャンケン」をすると相手は3つの選択肢を持ちます。<br>
　このように、`3個以上のイベント(出来事)があり、その確率の和が１になる問題をMultinomial問題`と考えましょう。　ここで、BinomialとBeta分布と同じよう、`Multinomial的なイベントを観察するなら、Dirichlet分布との組み合わせで計算を単純化させる`ことができます。

> 上の図「ジャンケン」の問題を見てみると、
1. 相手が何を出すか分からないので、今の知識は`Dirichlet(1,1,1)(=Uniform分布)`にする。
1. `Multinomial分布に従う`相手の選択を5回観察する。グー２、チョキ２、パー１
1. ただその観察を足し算して`Dirichlet(1+2,1+2,1+1)`でベイズ推定終了。

* **世の中の様々な出来ことって`BinomialよりはMultinomialに近い。`**
* **Multinomial分布は`Dirichlet分布とConjugation関係`であるので、**
* **Dirichlet分布を使うことで様々な観察の`推定がやりやすくなる。`**

<br><br><br>


## <a name="jump6">概念整理：LDA</a>
---
![LDA](/assets/img/dev/gibbs_8.jpg)
　LDAについて詳しく学ぶつもりではございません、ちょっとした結論から言うと`いくつかのサイコロを投げて、出た単語で文章を作る`という概念が`LDAへの一歩`です。サイコロっていう物には馴染みありますね…`サイコロを投げる → Multinomial分布 → Dirichlet分布で推定`に繋がると「Latent Dirichlet Allocation」`(可能性のあるDirichletを割り当てる。)`という意味が何と無く分かる気がします。
　7番スライドを見てみましょう。

> ある作家さんは「動物・愛・国」に関するテーマが得意であり、`「動物・愛・国」が書いている3面のサイコロ`を持っています。このサイコロは1/3の確率ではなく、作家さんが決めた確率で各面が出てきます。 また、100個の`同じ単語が書いている100面サイコロ`が3つあり、1個は「`動物`に関する単語がでる確率が高い」、1個は「`愛`に関する単語がでる確率が高い」、残り1個は「`国`に関する単語がよく出てきます。」
* 作家さんは本の執筆の前、`Topicサイコロの確率を調節`する。本で表したいTopicの確率を上げるはず。
   1. **Topicサイコロを投げ、1個のTopicを選ぶ。**
   1. **選ばれたTopicの単語サイコロを投げ、1個の単語を選ぶ。**
   1. **本に単語を書く。**
* 本になるまで1~3を繰り返す。(Topicサイコロの確率調節は本を書くたび最初1回だけ)<br><br>
Topicサイコロだけを1000回投げた後、その結果に従って単語サイコロを1000回投げる形になっても構わないと思います。

* **本の執筆作業は`いくつかの特別なサイコロを投げるプロセスであり、`**
* **`サイコロさえあれば、誰でも本を書ける。`**
* **好きな作家さんの`サイコロの形(確率)を真似すれば`似たような本を自ら書け、**
* **`サイコロの形を推定していく`のがLDAの概念である。**

<br><br><br>


## <a name="jump7">Gibbs Sampling</a>
---
![Gibbs1](/assets/img/dev/gibbs_9.jpg)
　そもそもGibbs Samplingのことを勉強するため書き始めた今回のプログですが、あっちこっち回って回って…やっと着きました。Gibbs SamplingはLDAで使われるサンプリング方法でありますので、`Gibbs Samplingが何かを学び、なぜGibbs Samplingじゃなければならないか`をいつものように`ザックリ`見てみましょう。

> Gibbs Samplingは`条件付き確率に用いたサンプリング方法`であり、前回紹介した[MCMC](#jump3)パラダイムの一種です。一気に全ての変数をサンプリングする訳ではなく、`1個づつ、他の変数は固定されていると思いながら`サンプリングが進みます。上の例では「x→y→z」の順番で進みましたが、順番はランダムで構いません。<br>
なんか「わけが分からない」と思うかもしれませんが、[山登りの例](#jump3)で考えてみると、`東ー西方向で今より高い所を探し移動する`後に`南ー北方向で高い所を探していく`仕組みになります。これ方法がLDAの中で何が素晴らしいですかね？`基本的なMCMC方法(Metropolis方法)と比較`してみます。

**LDAは単語の数の次元を扱う(めちゃくちゃ高次元・変数多い)**
* **普通のMCMCでは無数の方程式を解くので、`計算量がものたくさん増えるが、`**
* **Gibbs Samplingでは`条件付き確率か作れるのなら、計算が簡単である。`**
* **また、MCMCでサンプリングが進むためには`より良い変数を見つけなければならないが、`**
* **次元の呪い([Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))から`高次元でより良い所を探すのは相当難しい。`**
* **しかしGibbs Samplingでは、`一個づつ何と無く進める。`**

<br>


![Gibbs2](/assets/img/dev/gibbs_10.jpg)
　山登りではなく、[部屋の掃除の例](https://www.youtube.com/watch?v=BaM1uiCpj_E)という例を見つけたので、紹介しようと思います。LDAやGibbs Samplingに関する概念を理解するに良い動画であると思い、このブログを書くにも大変お世話になった資料です。

> 皆さんは上の汚い部屋を目にして、どう片付けようと思いますか？
* もしその部屋に住んでいる方であれば、「ボールはあっち、ぬいぐるみはこっち、…」、一気に全ての物を片付けれるかもしれません、
* しかし、普通にありえない問題であって、MCMC方法に従うと、`全ての物を少しづつ片付けたけど、それが間違った配置になると、またやり直さなきゃ！`という仕組みです。
* またもっと厳しくいうと、`「少しづつ片付ける」のは「ランダムに動かしてみる」`という行動になります。
* 普通なアクセスとして、「とにかく、ゴミはゴミ箱に入れる、後、マウスはパソコンの横に綺麗に、…」という方法がGibbs Sampling的な考えです。
* `この1個を除いて、みんな悪くない、そのままでいい`という考え方で、情報もはっきりして`別にランダムに動かす必要もないです。`


<br>


![Gibbs3](/assets/img/dev/gibbs_11.jpg)
　こんなに簡単なのと思いきや、疑問になるいくつかの点がありますした。
* `1個づつ、条件付き確率に用い変わった状況 ＝ 一気に全てが変わった状況`と言えるの？
* MCMCでは確率を計算し`AcceptしたりDenyしたりするけど、勝手に毎段階Accept`しちゃうの？

> しかし流石にその点についた数学的にはっきりした証明があり、こっちでは扱わないつもりですが、関連リンクを貼っておきます。
* [Gibbs Sampling関連Youtube動画](https://www.youtube.com/watch?v=fUim-eAdxDs&t=390s)
* [MCMC関連SlidePlayer資料](https://slideplayer.com/slide/4757282/)

<br><br><br>


## <a name="jump8">LDAへの適用</a>
---
![LDA概要](/assets/img/dev/gibbs_12.jpg)
　[前回説明した通り](#jump7)LDA推定にはGibbs Samplingが相応しいです。1000、10000個の単語があるとしても、1個づつその単語に関する情報を更新し続けるのができるからです。　実際に`LDAでGibbs Samplingがどんな風に動くかをザックリみてみましょう。`

> ある作家はサイコロを投げながら**<u>3つの本を作り</u>**ました。多分`本1は「動物・愛」を主なテーマに、本2は「愛・国」をテーマ`に考えたでしょう。具体的な本の作りかたは[こち](#jump6)を参考してください。　この中でLDAが最適化しようとするのは、
* **なるべく同じ本は同じ色(Topic)であり、**
* **同じ単語は同じ色(Topic)である**　ことです。

　上記の①〜③のよう、`1個の単語を黒色に変え、他の本・単語の情報をもとに改め色付ける`というプロセスを繰り返すって、確かに`Gibbs Sampling的な接近方`ですね…<br>
　最初はランダムに色ついた単語の情報から始まりますが、[サンプリングの理解](#jump3)で見たように、サンプリングを繰り返すことにより`作家のサイコロと似たような形に近付いていきます。`


**　最初申し上げたように、理論的に間違っている部分が色々あるかもしれません。コメントしていただければ、修正していきます。また、このブログをご覧になり、「LDAってこんなもんだ」とか「だからそういうことを勉強してたんだ」と考えてくださると、嬉しいです。**<br>
**　このホームページの最初の投稿になり、意外と長い投稿になってしまいましたが、これからはちょこちょこ得られた知識や考えたことを少しづつアップロードします。また、日本語の練習がてら日本語で書いていますが、それについたアドバイスもぜひお願いいたします。**

<br><br><br>


## <a name="jump9">参考資料</a>
---
* [강수네집、韓国語](http://www.ranoma.com/hard/54283)：累積確率関数
* [S/W 멤버십 기술 블로그、韓国語](http://www.ranoma.com/hard/54283)：MCMC
* [나의 큰 O는 log x야、韓国語](http://www.ranoma.com/hard/54283)：LDA
* [Youtube、英語](https://www.youtube.com/watch?v=fUim-eAdxDs&t=390s)：Gibbs Sampling
* [Slide Player、英語](https://slideplayer.com/slide/4757282/)：MCMC、Gibbs Sampling


